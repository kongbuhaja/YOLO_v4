{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import aug_utils, bbox_utils, anchor_utils\n",
    "from config import *\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, dtype, labels, batch_size, anchors, input_size, \n",
    "                 strides, positive_iou_threshold, max_bboxes, create_anchors):\n",
    "        self.dtype = dtype\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.col_anchors = len(anchors[0])\n",
    "        self.row_anchors = len(anchors)\n",
    "        self.num_classes = len(labels)\n",
    "        self.input_size = input_size\n",
    "        self.strides = np.array(strides)\n",
    "        self.scales = (self.input_size // np.array(self.strides)).tolist()\n",
    "        self.anchors = anchors\n",
    "        self.anchors_xywh = anchor_utils.get_anchors_xywh(anchors, self.strides, self.input_size)\n",
    "        self.positive_iou_threshold = positive_iou_threshold\n",
    "        self.max_bboxes = max_bboxes\n",
    "        self.create_anchors = create_anchors\n",
    "        self._length = {}\n",
    "\n",
    "    def __call__(self, split, use_tfrecord=True, use_label=False):\n",
    "        if self.dtype == 'voc':\n",
    "            from datasets.voc_dataset import Dataset\n",
    "        elif self.dtype == 'coco':\n",
    "            from datasets.coco_dataset import Dataset\n",
    "        elif self.dtype == 'custom':\n",
    "            from datasets.custom_dataset import Dataset\n",
    "        elif self.dtype == 'raw':\n",
    "            from datasets.raw_dataset import Dataset\n",
    "        dataset = Dataset(split, self.dtype, self.anchors, self.labels, self.input_size, self.create_anchors)\n",
    "\n",
    "        data = dataset.load(use_tfrecord)\n",
    "        self._length[split] = dataset.length\n",
    "\n",
    "        data = data.cache()\n",
    "\n",
    "        if split == 'train':\n",
    "            data = data.shuffle(buffer_size = min(self.length(split) * 3, 200000))\n",
    "            \n",
    "        # if you have enough ram move this line before data.cache(), it will be faster\n",
    "        data = data.map(self.tf_preprocessing, num_parallel_calls=-1) \n",
    "\n",
    "        if split == 'train':\n",
    "            data = data.map(aug_utils.tf_augmentation, num_parallel_calls=-1)\n",
    "            data = data.map(self.tf_minmax, num_parallel_calls=-1)\n",
    "        \n",
    "        data = data.map(lambda image, labels, width, height: aug_utils.tf_resize_padding(image, labels, width, height, self.input_size), num_parallel_calls=-1)\n",
    "        data = data.padded_batch(self.batch_size, padded_shapes=self.get_padded_shapes(), padding_values=self.get_padding_values(), drop_remainder=True)\n",
    "        \n",
    "        # data = data.map(lambda x, y: self.py_labels_to_grids(x, y, use_label), num_parallel_calls=-1).prefetch(1)\n",
    "        data = data.map(lambda image, labels: self.tf_encode(image, labels, use_label), num_parallel_calls=-1).prefetch(1)\n",
    "        return data\n",
    "    \n",
    "    def length(self, split):\n",
    "        return self._length[split]\n",
    "    \n",
    "    def py_encode(self, image, labels, use_label):\n",
    "        grids = tf.py_function(self.encode, [labels], [tf.float32]*self.row_anchors)\n",
    "        if use_label:\n",
    "            return image, *grids, labels\n",
    "        return image, *grids\n",
    "    \n",
    "    @tf.function\n",
    "    def tf_preprocessing(self, image, labels, width, height):\n",
    "        image = tf.cast(image, tf.float32)/255.\n",
    "        return image, labels, width, height\n",
    "    \n",
    "    def tf_minmax(self, image, labels, width, height):\n",
    "        return tf.maximum(tf.minimum(image, 1.), 0), labels, width, height\n",
    "    \n",
    "    @tf.function\n",
    "    def tf_resize_padding(self, image, labels, width, height):\n",
    "        image, labels = aug_utils.tf_resize_padding(image, labels, width, height, self.input_size)\n",
    "\n",
    "        return image, labels\n",
    "    \n",
    "    @tf.function\n",
    "    def tf_encode(self, image, labels, use_label):\n",
    "        grids = self.encode2(labels)\n",
    "        if use_label:\n",
    "            return image, *grids, labels\n",
    "        return image, *grids\n",
    "    \n",
    "    @tf.function\n",
    "    def onehot_label(self, prob, smooth=True, alpha=0.1):\n",
    "        onehot = tf.one_hot(tf.cast(prob, dtype=tf.int32), self.num_classes)\n",
    "        if smooth:\n",
    "            return onehot * (1. - alpha) + alpha/self.num_classes\n",
    "        return onehot\n",
    "\n",
    "    @tf.function\n",
    "    def encode(self, labels):\n",
    "        labels = bbox_utils.xyxy_to_xywh(labels, True)\n",
    "        conf = labels[..., 4:5]\n",
    "\n",
    "        onehot = conf * self.onehot_label(labels[..., 5], smooth=True)\n",
    "\n",
    "        grids = []\n",
    "        anchor_xy = [tf.reshape(anchor[..., :2], [-1,2]) for anchor in self.anchors_xywh]\n",
    "        anchor_wh = [tf.reshape(anchor[..., 2:], [-1,2]) for anchor in self.anchors_xywh]\n",
    "\n",
    "        center_anchors = tf.concat([tf.concat([anchor_xy[i] + 0.5, anchor_wh[i]], -1) * self.strides[i] for i in range(self.row_anchors)], 0)\n",
    "\n",
    "        ious = bbox_utils.bbox_iou(center_anchors[:, None], labels[:, None, ..., :4])\n",
    "\n",
    "        # assign maximum label\n",
    "        maximum_positive_ious = tf.where(tf.greater_equal(ious, self.positive_iou_threshold), ious, 0.)\n",
    "\n",
    "        # assign minimum label\n",
    "        best_anchor_iou = tf.reduce_max(ious, -2, keepdims=True)\n",
    "        minimum_positive_ious = tf.where(tf.logical_and(ious == best_anchor_iou, ious > 0), best_anchor_iou, 0.)\n",
    "    \n",
    "        # join minimum, maximum label\n",
    "        joined_ious = tf.where(tf.cast(minimum_positive_ious, tf.bool), minimum_positive_ious, maximum_positive_ious)\n",
    "        joined_positive_mask = tf.cast(tf.reduce_any(tf.cast(joined_ious, tf.bool), -1, keepdims=True), tf.float32)\n",
    "\n",
    "        assigned_labels = tf.gather(tf.concat([labels[..., :5], onehot],-1), tf.argmax(joined_ious, -1), batch_dims=1) * joined_positive_mask\n",
    "\n",
    "        for i in range(self.row_anchors):\n",
    "            scale = self.scales[i]\n",
    "            start = 0 if i==0 else tf.reduce_sum((self.input_size//self.strides[:i])**2 * self.col_anchors)\n",
    "            end = start + (scale)**2 * self.col_anchors\n",
    "            grids += [tf.reshape(assigned_labels[:, start:end], [self.batch_size, scale, scale, self.col_anchors, -1])]\n",
    "\n",
    "        return grids\n",
    "\n",
    "    @tf.function\n",
    "    def encode2(self, labels):\n",
    "        labels = bbox_utils.xyxy_to_xywh(labels, True)\n",
    "        conf = labels[..., 4:5]\n",
    "\n",
    "        onehot = conf * self.onehot_label(labels[..., 5], smooth=True)\n",
    "\n",
    "        grids = []\n",
    "        anchor_xy = [tf.reshape(anchor[..., :2], [-1,2]) for anchor in self.anchors_xywh]\n",
    "        anchor_wh = [tf.reshape(anchor[..., 2:], [-1,2]) for anchor in self.anchors_xywh]\n",
    "\n",
    "        center_anchors = tf.concat([tf.concat([anchor_xy[i] + 0.5, anchor_wh[i]], -1) * self.strides[i] for i in range(self.row_anchors)], 0)\n",
    "\n",
    "        ious = bbox_utils.bbox_iou(center_anchors[:, None], labels[:, None, ..., :4])\n",
    "\n",
    "        # assign maximum label\n",
    "        maximum_positive_ious = tf.where(tf.greater_equal(ious, self.positive_iou_threshold), ious, 0.)     \n",
    "\n",
    "        # assign minimum label\n",
    "        best_anchor_iou = tf.reduce_max(ious, -2, keepdims=True)\n",
    "        minimum_positive_ious = tf.where(tf.logical_and(ious == best_anchor_iou, ious > 0), best_anchor_iou, 0.)\n",
    "    \n",
    "        # join minimum, maximum label\n",
    "        joined_ious = tf.where(tf.cast(minimum_positive_ious, tf.bool), minimum_positive_ious, maximum_positive_ious)\n",
    "        joined_positive_mask = tf.cast(tf.reduce_any(tf.cast(joined_ious, tf.bool), -1, keepdims=True), tf.float32)\n",
    "\n",
    "        assigned_labels = tf.gather(tf.concat([labels[..., :5], onehot],-1), tf.argmax(joined_ious, -1), batch_dims=1) * joined_positive_mask\n",
    "        grid_xy = tf.concat([anchor_xy[i] for i in range(self.row_anchors)], 0)\n",
    "        tiled_strides = tf.cast(tf.concat([tf.tile(stride[None, None, None], [self.batch_size, scale**2*self.col_anchors, 1]) for stride, scale in zip(self.strides, self.scales)], 1), tf.float32)\n",
    "        based_xy = (grid_xy[None] - tf.minimum(tf.maximum(grid_xy[None] - assigned_labels[..., :2] / tiled_strides, -1), 0)) * joined_positive_mask * tiled_strides\n",
    "        assigned_labels = tf.concat([based_xy, assigned_labels[..., 2:]], -1)\n",
    "\n",
    "        for i in range(self.row_anchors):\n",
    "            scale = self.scales[i]\n",
    "            start = 0 if i==0 else tf.reduce_sum((self.input_size//self.strides[:i])**2 * self.col_anchors)\n",
    "            end = start + (scale)**2 * self.col_anchors\n",
    "            grids += [tf.reshape(assigned_labels[:, start:end], [self.batch_size, scale, scale, self.col_anchors, -1])]\n",
    "\n",
    "        return grids\n",
    "    \n",
    "    @tf.function\n",
    "    def get_padded_shapes(self):\n",
    "        return [None, None, None], [self.max_bboxes, None]\n",
    "\n",
    "    @tf.function\n",
    "    def get_padding_values(self):\n",
    "        return tf.constant(0, tf.float32), tf.constant(0, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 02:42:16.295967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.311012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.311151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.311765: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-04 02:42:16.312206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.312313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.312404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.593594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.593729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.593829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.593927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18083 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0e:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from config import *\n",
    "input_size = 1280\n",
    "dataloader = DataLoader(DTYPE, LABELS, BATCH_SIZE, ANCHORS, input_size, [8,16,32], 0.5, 100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hs/anaconda3/envs/tf28/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: custom val\n",
      "./data/custom/val.tfrecord is exist\n"
     ]
    }
   ],
   "source": [
    "data = dataloader('val', use_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = next(iter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, s, m, l, gts = d\n",
    "s_color = (255, 0, 0)\n",
    "m_color = (0, 255, 0)\n",
    "l_color = (0, 0, 255)\n",
    "gt_color = (0, 0, 0)\n",
    "\n",
    "index = 0\n",
    "image = i.numpy()[index][..., ::-1]*255\n",
    "small = s.numpy()[index]\n",
    "medium = m.numpy()[index]\n",
    "large = l.numpy()[index]\n",
    "gt = gts.numpy()[index]\n",
    "group = [[small, s_color], [medium, m_color], [large, l_color], [gt, gt_color]]\n",
    "\n",
    "\n",
    "s = 8\n",
    "# for i in range(0, input_size, s):\n",
    "#     cv2.line(image, (0, i), (input_size, i), (255,255,255, 1))\n",
    "#     cv2.line(image, (i, 0), (i, input_size), (255,255,255, 1))\n",
    "    \n",
    "for i, (sml, color) in enumerate(group):\n",
    "    for label in sml.reshape([-1, sml.shape[-1]]):\n",
    "        box = label[:4]\n",
    "        if i!=3:\n",
    "            box = bbox_utils.xywh_to_xyxy(box).numpy()\n",
    "        if i in [10]:\n",
    "            break\n",
    "        wh = ((box[:2] + box[2:]) * 0.5).astype(np.int32)\n",
    "        box = box.astype(np.int32)\n",
    "        \n",
    "        if label[5]==0:\n",
    "            continue\n",
    "        cv2.rectangle(image, box[:2], box[2:], color, 1)\n",
    "        \n",
    "        cv2.circle(image, wh, 2, color, 1)\n",
    "        # print(box)\n",
    "\n",
    "# cv2.imshow('image', image.astype(np.uint8))\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.resize(image, (1248,1248))\n",
    "\n",
    "cv2.imshow('image', image.astype(np.uint8))\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_utils import load_model\n",
    "from models.backbones import *\n",
    "from models.necks import *\n",
    "from models.heads import *\n",
    "from config import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import GlorotUniform as glorot\n",
    "# model, _, _, _, _ = load_model(MODEL_TYPE, ANCHORS, NUM_CLASSES, STRIDES, IOU_THRESHOLD, EPS, INF, KERNEL_INITIALIZER, LOAD_CHECKPOINTS, CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.zeros((1, 512,512,3))\n",
    "scale = [64,32,16]\n",
    "backbone = CSPDarknet53(activate='Mish', scaled=True, kernel_initializer=glorot)\n",
    "panspp = PANSPP(512, layer_size=3, block_size=2, branch_transition=True,\n",
    "                               activate='LeakyReLU', kernel_initializer=glorot)\n",
    "csppanspp = CSPPANSPP(512, layer_size=3, block_size=2, branch_transition=True,\n",
    "                                  activate='Mish', kernel_initializer=glorot)\n",
    "head = YoloHead(256, scale, 3, 80, 'Mish', kernel_initializer=glorot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = backbone(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = panspp(temp)\n",
    "h1 = head(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2 = csppanspp(temp)\n",
    "h2 = head(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 16, 16, 3, 85), dtype=float32, numpy=\n",
       "array([[[[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]]]]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bbox_utils import bbox_iou\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "a=np.zeros((4))[None]\n",
    "b=np.zeros((0, 4))\n",
    "iou = bbox_iou(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.zeros((0,4))\n",
    "while(a.shape[0]):\n",
    "    print(a.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 12:24:56.551313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-07 12:24:56.566462: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-07 12:24:56.566540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-07 12:24:56.567005: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-07 12:24:56.567568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-07 12:24:56.567621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-07 12:24:56.567659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-07 12:24:56.828152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-07 12:24:56.828237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-07 12:24:56.828285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-07 12:24:56.828343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21720 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from utils import data_utils, train_utils, io_utils, eval_utils, post_processing, anchor_utils, bbox_utils\n",
    "from config import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "tf.random.set_seed(42)\n",
    "dataloader = data_utils.DataLoader(DTYPE, LABELS, BATCH_SIZE, ANCHORS, 416, \n",
    "                                       [8,16,32], POSITIVE_IOU_THRESHOLD, MAX_BBOXES, CREATE_ANCHORS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dblab/anaconda3/envs/tf28/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: voc train\n",
      "./data/voc/train.tfrecord is exist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = dataloader('train', use_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = next(iter(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 100, 6), dtype=float32, numpy=\n",
       "array([[[ 87.,  -1., 381., 383.,   1.,  12.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        ...,\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "       [[ 96., 196., 198., 255.,   1.,  13.],\n",
       "        [118., 197., 228., 254.,   1.,  14.],\n",
       "        [182., 185., 250., 204.,   1.,  14.],\n",
       "        ...,\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "       [[187., 237., 205., 261.,   1.,  19.],\n",
       "        [217., 323., 230., 336.,   1.,  19.],\n",
       "        [226., 384., 238., 401.,   1.,  19.],\n",
       "        ...,\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[265., 339., 277., 411.,   1.,   3.],\n",
       "        [248.,   4., 265.,  15.,   1.,  14.],\n",
       "        [263., 312., 270., 339.,   1.,   3.],\n",
       "        ...,\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "       [[ 33.,  69., 414., 284.,   1.,  11.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        ...,\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.]],\n",
       "\n",
       "       [[234., 117., 348., 212.,   1.,  19.],\n",
       "        [292., 286., 415., 416.,   1.,   8.],\n",
       "        [256.,   1., 415., 416.,   1.,  14.],\n",
       "        ...,\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.],\n",
       "        [  0.,   0.,   0.,   0.,   0.,   0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, s, m, l, gt = tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (i[0] * 255).numpy().astype(np.uint8)\n",
    "label = gt[0].numpy().astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(image, label[0][:2], label[0][2:4], (255,255, 0), 1)\n",
    "cv2.imshow('image', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 416, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.6645621, shape=(), dtype=float32)\n",
      "tf.Tensor(0.6645621, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "print(tf.random.uniform(()))\n",
    "tf.random.set_seed(42)\n",
    "print(tf.random.uniform(()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.7413678, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.random.uniform(()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = np.zeros((10,4))\n",
    "bboxes[0] = [10,10,30,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ro(bboxes):\n",
    "    bboxes = np.stack([bboxes[..., 1], \n",
    "                      100 - bboxes[..., 2] -1,\n",
    "                      bboxes[..., 3],\n",
    "                      100 - bboxes[..., 0] -1], -1)\n",
    "    return bboxes\n",
    "def fh(bboxes):\n",
    "    bboxes = np.stack([100 - bboxes[..., 2] - 1,\n",
    "                    bboxes[..., 1],\n",
    "                    100 - bboxes[..., 0] - 1,\n",
    "                    bboxes[..., 3]], -1)\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = ro(bboxes)\n",
    "bboxes = fh(bboxes)\n",
    "bboxes = ro(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[69., 49., 89., 89.],\n",
       "       [99., 99., 99., 99.],\n",
       "       [99., 99., 99., 99.],\n",
       "       [99., 99., 99., 99.],\n",
       "       [99., 99., 99., 99.],\n",
       "       [99., 99., 99., 99.],\n",
       "       [99., 99., 99., 99.],\n",
       "       [99., 99., 99., 99.],\n",
       "       [99., 99., 99., 99.],\n",
       "       [99., 99., 99., 99.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. 파인더 일반제품 1개, 대용량 배터리 1개 총 2가지 구매하려고 하는데 견적서 필요한 서류가 있을까요?\n",
    "2. 매달 사용료가 있는데 이부분은 어떻게 결제를 진행하면 될까요? 견적서에 포함해서 드리면 될까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf28",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
