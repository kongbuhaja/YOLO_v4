{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import aug_utils, bbox_utils, anchor_utils\n",
    "from config import *\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, dtype, labels, batch_size, anchors, input_size, \n",
    "                 strides, positive_iou_threshold, max_bboxes, create_anchors):\n",
    "        self.dtype = dtype\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.col_anchors = len(anchors[0])\n",
    "        self.row_anchors = len(anchors)\n",
    "        self.num_classes = len(labels)\n",
    "        self.input_size = input_size\n",
    "        self.strides = np.array(strides)\n",
    "        self.scales = (self.input_size // np.array(self.strides)).tolist()\n",
    "        self.anchors = anchors\n",
    "        self.anchors_xywh = anchor_utils.get_anchors_xywh(anchors, self.strides, self.input_size)\n",
    "        self.positive_iou_threshold = positive_iou_threshold\n",
    "        self.max_bboxes = max_bboxes\n",
    "        self.create_anchors = create_anchors\n",
    "        self._length = {}\n",
    "\n",
    "    def __call__(self, split, use_tfrecord=True, use_label=False):\n",
    "        if self.dtype == 'voc':\n",
    "            from datasets.voc_dataset import Dataset\n",
    "        elif self.dtype == 'coco':\n",
    "            from datasets.coco_dataset import Dataset\n",
    "        elif self.dtype == 'custom':\n",
    "            from datasets.custom_dataset import Dataset\n",
    "        elif self.dtype == 'raw':\n",
    "            from datasets.raw_dataset import Dataset\n",
    "        dataset = Dataset(split, self.dtype, self.anchors, self.labels, self.input_size, self.create_anchors)\n",
    "\n",
    "        data = dataset.load(use_tfrecord)\n",
    "        self._length[split] = dataset.length\n",
    "\n",
    "        data = data.cache()\n",
    "\n",
    "        if split == 'train':\n",
    "            data = data.shuffle(buffer_size = min(self.length(split) * 3, 200000))\n",
    "            \n",
    "        # if you have enough ram move this line before data.cache(), it will be faster\n",
    "        data = data.map(self.tf_preprocessing, num_parallel_calls=-1) \n",
    "\n",
    "        if split == 'train':\n",
    "            data = data.map(aug_utils.tf_augmentation, num_parallel_calls=-1)\n",
    "            data = data.map(self.tf_minmax, num_parallel_calls=-1)\n",
    "        \n",
    "        data = data.map(lambda image, labels, width, height: aug_utils.tf_resize_padding(image, labels, width, height, self.input_size), num_parallel_calls=-1)\n",
    "        data = data.padded_batch(self.batch_size, padded_shapes=self.get_padded_shapes(), padding_values=self.get_padding_values(), drop_remainder=True)\n",
    "        \n",
    "        # data = data.map(lambda x, y: self.py_labels_to_grids(x, y, use_label), num_parallel_calls=-1).prefetch(1)\n",
    "        data = data.map(lambda image, labels: self.tf_encode(image, labels, use_label), num_parallel_calls=-1).prefetch(1)\n",
    "        return data\n",
    "    \n",
    "    def length(self, split):\n",
    "        return self._length[split]\n",
    "    \n",
    "    def py_encode(self, image, labels, use_label):\n",
    "        grids = tf.py_function(self.encode, [labels], [tf.float32]*self.row_anchors)\n",
    "        if use_label:\n",
    "            return image, *grids, labels\n",
    "        return image, *grids\n",
    "    \n",
    "    @tf.function\n",
    "    def tf_preprocessing(self, image, labels, width, height):\n",
    "        image = tf.cast(image, tf.float32)/255.\n",
    "        return image, labels, width, height\n",
    "    \n",
    "    def tf_minmax(self, image, labels, width, height):\n",
    "        return tf.maximum(tf.minimum(image, 1.), 0), labels, width, height\n",
    "    \n",
    "    @tf.function\n",
    "    def tf_resize_padding(self, image, labels, width, height):\n",
    "        image, labels = aug_utils.tf_resize_padding(image, labels, width, height, self.input_size)\n",
    "\n",
    "        return image, labels\n",
    "    \n",
    "    @tf.function\n",
    "    def tf_encode(self, image, labels, use_label):\n",
    "        grids = self.encode2(labels)\n",
    "        if use_label:\n",
    "            return image, *grids, labels\n",
    "        return image, *grids\n",
    "    \n",
    "    @tf.function\n",
    "    def onehot_label(self, prob, smooth=True, alpha=0.1):\n",
    "        onehot = tf.one_hot(tf.cast(prob, dtype=tf.int32), self.num_classes)\n",
    "        if smooth:\n",
    "            return onehot * (1. - alpha) + alpha/self.num_classes\n",
    "        return onehot\n",
    "\n",
    "    @tf.function\n",
    "    def encode(self, labels):\n",
    "        labels = bbox_utils.xyxy_to_xywh(labels, True)\n",
    "        conf = labels[..., 4:5]\n",
    "\n",
    "        onehot = conf * self.onehot_label(labels[..., 5], smooth=True)\n",
    "\n",
    "        grids = []\n",
    "        anchor_xy = [tf.reshape(anchor[..., :2], [-1,2]) for anchor in self.anchors_xywh]\n",
    "        anchor_wh = [tf.reshape(anchor[..., 2:], [-1,2]) for anchor in self.anchors_xywh]\n",
    "\n",
    "        center_anchors = tf.concat([tf.concat([anchor_xy[i] + 0.5, anchor_wh[i]], -1) * self.strides[i] for i in range(self.row_anchors)], 0)\n",
    "\n",
    "        ious = bbox_utils.bbox_iou(center_anchors[:, None], labels[:, None, ..., :4])\n",
    "\n",
    "        # assign maximum label\n",
    "        maximum_positive_ious = tf.where(tf.greater_equal(ious, self.positive_iou_threshold), ious, 0.)\n",
    "\n",
    "        # assign minimum label\n",
    "        best_anchor_iou = tf.reduce_max(ious, -2, keepdims=True)\n",
    "        minimum_positive_ious = tf.where(tf.logical_and(ious == best_anchor_iou, ious > 0), best_anchor_iou, 0.)\n",
    "    \n",
    "        # join minimum, maximum label\n",
    "        joined_ious = tf.where(tf.cast(minimum_positive_ious, tf.bool), minimum_positive_ious, maximum_positive_ious)\n",
    "        joined_positive_mask = tf.cast(tf.reduce_any(tf.cast(joined_ious, tf.bool), -1, keepdims=True), tf.float32)\n",
    "\n",
    "        assigned_labels = tf.gather(tf.concat([labels[..., :5], onehot],-1), tf.argmax(joined_ious, -1), batch_dims=1) * joined_positive_mask\n",
    "\n",
    "        for i in range(self.row_anchors):\n",
    "            scale = self.scales[i]\n",
    "            start = 0 if i==0 else tf.reduce_sum((self.input_size//self.strides[:i])**2 * self.col_anchors)\n",
    "            end = start + (scale)**2 * self.col_anchors\n",
    "            grids += [tf.reshape(assigned_labels[:, start:end], [self.batch_size, scale, scale, self.col_anchors, -1])]\n",
    "\n",
    "        return grids\n",
    "\n",
    "    @tf.function\n",
    "    def encode2(self, labels):\n",
    "        labels = bbox_utils.xyxy_to_xywh(labels, True)\n",
    "        conf = labels[..., 4:5]\n",
    "\n",
    "        onehot = conf * self.onehot_label(labels[..., 5], smooth=True)\n",
    "\n",
    "        grids = []\n",
    "        anchor_xy = [tf.reshape(anchor[..., :2], [-1,2]) for anchor in self.anchors_xywh]\n",
    "        anchor_wh = [tf.reshape(anchor[..., 2:], [-1,2]) for anchor in self.anchors_xywh]\n",
    "\n",
    "        center_anchors = tf.concat([tf.concat([anchor_xy[i] + 0.5, anchor_wh[i]], -1) * self.strides[i] for i in range(self.row_anchors)], 0)\n",
    "\n",
    "        ious = bbox_utils.bbox_iou(center_anchors[:, None], labels[:, None, ..., :4])\n",
    "\n",
    "        # assign maximum label\n",
    "        maximum_positive_ious = tf.where(tf.greater_equal(ious, self.positive_iou_threshold), ious, 0.)     \n",
    "\n",
    "        # assign minimum label\n",
    "        best_anchor_iou = tf.reduce_max(ious, -2, keepdims=True)\n",
    "        minimum_positive_ious = tf.where(tf.logical_and(ious == best_anchor_iou, ious > 0), best_anchor_iou, 0.)\n",
    "    \n",
    "        # join minimum, maximum label\n",
    "        joined_ious = tf.where(tf.cast(minimum_positive_ious, tf.bool), minimum_positive_ious, maximum_positive_ious)\n",
    "        joined_positive_mask = tf.cast(tf.reduce_any(tf.cast(joined_ious, tf.bool), -1, keepdims=True), tf.float32)\n",
    "\n",
    "        assigned_labels = tf.gather(tf.concat([labels[..., :5], onehot],-1), tf.argmax(joined_ious, -1), batch_dims=1) * joined_positive_mask\n",
    "        grid_xy = tf.concat([anchor_xy[i] for i in range(self.row_anchors)], 0)\n",
    "        tiled_strides = tf.cast(tf.concat([tf.tile(stride[None, None, None], [self.batch_size, scale**2*self.col_anchors, 1]) for stride, scale in zip(self.strides, self.scales)], 1), tf.float32)\n",
    "        based_xy = (grid_xy[None] - tf.minimum(tf.maximum(grid_xy[None] - assigned_labels[..., :2] / tiled_strides, -1), 0)) * joined_positive_mask * tiled_strides\n",
    "        assigned_labels = tf.concat([based_xy, assigned_labels[..., 2:]], -1)\n",
    "\n",
    "        for i in range(self.row_anchors):\n",
    "            scale = self.scales[i]\n",
    "            start = 0 if i==0 else tf.reduce_sum((self.input_size//self.strides[:i])**2 * self.col_anchors)\n",
    "            end = start + (scale)**2 * self.col_anchors\n",
    "            grids += [tf.reshape(assigned_labels[:, start:end], [self.batch_size, scale, scale, self.col_anchors, -1])]\n",
    "\n",
    "        return grids\n",
    "    \n",
    "    @tf.function\n",
    "    def get_padded_shapes(self):\n",
    "        return [None, None, None], [self.max_bboxes, None]\n",
    "\n",
    "    @tf.function\n",
    "    def get_padding_values(self):\n",
    "        return tf.constant(0, tf.float32), tf.constant(0, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 02:42:16.295967: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.311012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.311151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.311765: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-04 02:42:16.312206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.312313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.312404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.593594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.593729: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.593829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 02:42:16.593927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18083 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0e:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from config import *\n",
    "input_size = 1280\n",
    "dataloader = DataLoader(DTYPE, LABELS, BATCH_SIZE, ANCHORS, input_size, [8,16,32], 0.5, 100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hs/anaconda3/envs/tf28/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: custom val\n",
      "./data/custom/val.tfrecord is exist\n"
     ]
    }
   ],
   "source": [
    "data = dataloader('val', use_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = next(iter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, s, m, l, gts = d\n",
    "s_color = (255, 0, 0)\n",
    "m_color = (0, 255, 0)\n",
    "l_color = (0, 0, 255)\n",
    "gt_color = (0, 0, 0)\n",
    "\n",
    "index = 0\n",
    "image = i.numpy()[index][..., ::-1]*255\n",
    "small = s.numpy()[index]\n",
    "medium = m.numpy()[index]\n",
    "large = l.numpy()[index]\n",
    "gt = gts.numpy()[index]\n",
    "group = [[small, s_color], [medium, m_color], [large, l_color], [gt, gt_color]]\n",
    "\n",
    "\n",
    "s = 8\n",
    "# for i in range(0, input_size, s):\n",
    "#     cv2.line(image, (0, i), (input_size, i), (255,255,255, 1))\n",
    "#     cv2.line(image, (i, 0), (i, input_size), (255,255,255, 1))\n",
    "    \n",
    "for i, (sml, color) in enumerate(group):\n",
    "    for label in sml.reshape([-1, sml.shape[-1]]):\n",
    "        box = label[:4]\n",
    "        if i!=3:\n",
    "            box = bbox_utils.xywh_to_xyxy(box).numpy()\n",
    "        if i in [10]:\n",
    "            break\n",
    "        wh = ((box[:2] + box[2:]) * 0.5).astype(np.int32)\n",
    "        box = box.astype(np.int32)\n",
    "        \n",
    "        if label[5]==0:\n",
    "            continue\n",
    "        cv2.rectangle(image, box[:2], box[2:], color, 1)\n",
    "        \n",
    "        cv2.circle(image, wh, 2, color, 1)\n",
    "        # print(box)\n",
    "\n",
    "# cv2.imshow('image', image.astype(np.uint8))\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.resize(image, (1248,1248))\n",
    "\n",
    "cv2.imshow('image', image.astype(np.uint8))\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf28",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
